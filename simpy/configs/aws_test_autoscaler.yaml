# An unique identifier for the head node and workers of this cluster.
cluster_name: lairning

# The maximum number of workers nodes to launch in addition to the head
# node. This takes precedence over min_workers. min_workers default to 0.
min_workers: 0

max_workers: 2

# The autoscaler will scale up the cluster faster with higher upscaling speed.
# E.g., if the task requires adding more nodes then autoscaler will gradually
# scale up the cluster in chunks of upscaling_speed*currently_running_nodes.
# This number should be > 0.
upscaling_speed: 1.0

# docker:
#    image: "rayproject/ray-ml:latest-cpu" # You can change this to latest-cpu if you don't need GPU support and want a faster startup
    # image: rayproject/ray:latest-gpu   # use this one if you don't need ML dependencies, it's faster to pull
#    container_name: "ray_container"

# If a node is idle for this many minutes, it will be removed.
idle_timeout_minutes: 5


# Cloud-provider specific configuration.
provider:
    type: aws
    region: eu-west-1
    availability_zone: eu-west-1a

# How Ray will authenticate with newly launched nodes.
auth:
    ssh_user: ubuntu

# Provider-specific config for the head node, e.g. instance type.
head_node:
    InstanceType: m5.large
    ImageId: ami-017849919db4eac7c # amazon/Deep Learning AMI (Ubuntu 18.04) Version 40.0

# Provider-specific config for worker nodes, e.g. instance type.
worker_nodes:
    InstanceType: m5.large
    ImageId: ami-017849919db4eac7c # amazon/Deep Learning AMI (Ubuntu 18.04) Version 40.0
    InstanceMarketOptions:
        MarketType: spot

file_mounts: {
    '/home/ubuntu/trainer': '~/drl-trainers/simpy/trainer_traffic_light_azure',
}

# List of shell commands to run to set up nodes.
setup_commands:
    - pip install torch==1.8.1+cpu -f https://download.pytorch.org/whl/torch_stable.html
    - pip install -U 'ray[rllib]'
    - pip install simpy seaborn
